# Optical Flow & Hand Tracking Benchmark

This project explores **hand tracking in video** and **optical flow estimation** using both classical computer vision algorithms and deep learning models.  
Developed as part of the **Image, Audio and Video Processing (PIAV)** course, it benchmarks 5 approaches on the same data and provides a comparative analysis of **position** and **velocity**.

ğŸ“„ A detailed summary of the work is also available in the [project poster](Poster_OpticalFlow.pdf).


## ğŸ¯ Objectives
- Track the **right hand** in the `people2` video using different algorithms.  
- Compare **position stability** and **velocity estimates** across methods.  
- Apply the same algorithms to the **FlyingChairs dataset** and evaluate against ground truth.


## ğŸ§© Methods implemented
- **YOLOv8-Pose** â†’ detects people and tracks keypoint 10 (right hand).  
- **Lucasâ€“Kanade (manual implementation)** â†’ optical flow from first principles (Sobel gradients, normal equations).  
- **Lucasâ€“Kanade (OpenCV)** â†’ efficient sparse tracking with pyramids.  
- **FarnebÃ¤ck** â†’ dense optical flow, sampled at the keypoint.  
- **RAFT** â†’ state-of-the-art deep learning optical flow model (torchvision pretrained).  

Velocity is computed as **Euclidean displacement Ã— FPS**, allowing fair comparison across methods.


## ğŸ“Š Results at a glance
- **YOLOv8-Pose**: stable positions, higher and more dynamic velocity estimates.  
- **RAFT / FarnebÃ¤ck / LK OpenCV**: consistent trajectories with moderate velocity fluctuations.  
- **LK Manual**: noticeable deviations, highlighting precision limitations of the hand-coded approach.  

On **FlyingChairs**, RAFT achieves results closest to the ground truth, with FarnebÃ¤ck and LK producing reasonable but less precise flow fields.


## ğŸ› ï¸ Tech stack
- **Python**  
- **PyTorch** + **torchvision (RAFT)**  
- **Ultralytics YOLOv8-Pose**  
- **OpenCV** (Lucasâ€“Kanade, FarnebÃ¤ck, video I/O)  
- **NumPy, Matplotlib**  
- **Google Colab** (GPU execution)


## ğŸš€ How to use
1. Clone this repository:
   ```bash
   git clone https://github.com/alebola/optical-flow-tracking-benchmark.git
   cd optical-flow-tracking-benchmark
   ```
2. Install dependencies:
   ```bash
   pip install ultralytics opencv-python torch torchvision matplotlib numpy
   ```
3. Open and run the notebook:
   ```bash
   notebooks/optical_flow_tracking.ipynb
   ```
4. Required inputs:
   - videos/people2.mp4
   - A sample from the FlyingChairs dataset (00005_img1.ppm, 00005_img2.ppm, 00005_flow.flo)


## ğŸ“ Notes
- This repository only includes a small sample of FlyingChairs (5 pairs). The full dataset (22,872 pairs) can be downloaded from the official source [FlyingChairs dataset](https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html). 
- This project was developed jointly with [David GarcÃ­a DÃ­az](https://github.com/Davidgrcdz) as part of the **Image, Audio and Video Processing (PIAV)** course.


